# file-stats
Some tools for attributing filesystem usage

catalog_disk_usage.py  - scans whole tree. also generates aggregate totals for directories on size, count, dates.  The per-file listing this generates feeds into the other scripts.  Note that the file paths listed are well sanitized, with tabs, non-ascii etc escaped in a reversible way.  Generates .files.txt and .dirs.txt from scanning filesystem.  Sorted by path.

Summarize.py - aggregates by all, individual user, file extension. TBD aggregate by group.  Also gives stats breaking down file sizes, access ages. Small enough one can open in Excel, but a bunch of useful details for figuring out what's on the disk.  Generates .summ.txt from .files.txt.

annotate_scan.py - adds extra columns to the scan files, which help narrow down on which files and directories consume the bulk of the space. Generates .annot.files.txt from .files.txt and .annot.dirs.txt from .dirs.txt, sorted by size.

extract_dir_stats.py - computes more stats on directories.  Includes scores that are high for "interesting" directories, eg much older than its peers, or suddenly fan out.  Includes the stats computed by 'crawlinator'.  Generates .dirs.txt from .files.txt, but has more columns than the one generated by catalog_disk_usage.py.  Sorted by path.

High level description of files:

.files.txt  - This is the raw, file-by-file catalog of the contents of the filesystem.  It's been extended to include "groupname" as well as "group_readable" and "all_readable" indicating whether the group and other read bits are set on the file.

.dirs.txt - This is a list of all directories, created during the catalog process. Line has a directory and the total size of the files below it as well as the most recent access time and modification time of files below it.  This may allow you to quickly identify "dead branches", though it's a little hard to navigate the file to find the places in the hierarchy where the times start to diverge.

.annot.files.txt - a list of files, sorted by size.  Includes the readability elements (all and group), as well as information about whether it's a "dup" (which means a "hard link" so actually the same file as another rather than "has the same contents").  There is also, for each file a "fract"ion of the total space, a "cum"ulative "fract"ion, and "cumfractcat" which allows you to see what files would need to be deleted to clear 80, 90, 95, 99% of the disk (by deleting the largest files).  It also begins to expose the file extensions, attempting to pick off as many as three parts of the name.

.annot.dirs.txt - a list of directories, sorted by size.  Each one has the most recent access time and most recent modification time for the contents of the directory.  Also the same fraction and cumulative fraction.  The thing to take away from this that the cumulative fractions are a bit weird...they really mean e.g. 80%+.  So if you deleted all the ones labeled 80 you'd free more than 80%.  The primary benefit is that it lets you see what needs to go if you want to reach a particular target free space.  

.summ.txt  the per user summary from before with some extensions.  Most noteably is that any file extension with more than 100 files gets summarized as though it was another user (see the "zz.ext.ext" lines at the end of the file.  At the beginning of the file there is a fake user named _ALL_ which has the statistics for all files to give you an idea about how the space breaks down overall based on age and file size



More detailed description of files:

summ.txt files:
* user    username   -- login name, _ALL_, zz.fileext1, zz.fileext1.fileext2, zz.fileext1.fileext2.fileext3 -- category that the rest of the line is aggregated over. file is sorted by "user".  Note .gz and .tar.gz files will sort adjacently, as they are represented as zz.gz and zz.gz.tar. 
* fileCnt fileSize        TBfileSize      Last access     Last modified   -- aggregate stats for count, size, and dates within the category
* Cnt<1M  Cnt<1G  Cnt<10G Cnt<100G   Cnt<1T   Cnt>1T  -- how many files are in each size bin. bins are disjoint, not cumulative
* TB<1M   TB<1G   TB<10G  TB<100G TB<1T   TB>1T   -- total size (in TB) taken up by files of a certain size
* Cntcsv  Cntbam  Cntgz   Cnttar  TBcsv   TBbam   TBgz    TBtar    -- counts and sizes of certain common extensions
* Cnta<1w     Cnta<1m Cnta<1q Cnta<1y Cnta<2y Cnta>2y  -- how many files were accessed this much before the scan date.  bins are disjoint, not cumulative
* TBa<1w  TBa<1m  TBa<1q  TBa<1y  TBa<2y  TBa>2y   -- size (in TB) of the files accessed this much before the scan date
* Size<1M Size<1G Size<10G        Size<100G   Size<1T Size>1T -- total size (in bytes) of files of a certain size 
* Sizecsv Sizebam Sizegz  Sizetar  -- total size (in bytes) of files of certain common extensions
* Sizea<1w        Sizea<1m        Sizea<1q        Sizea<1y        Sizea<2y   Sizea>2y -- total size (in bytes) of files accessed this much before the scan date

annot.dirs.txt files:
* dir     - directory path
* size    files   last_access     last_modified -- stats aggregated over the directory and all of its children.  Size is in bytes.   File is sorted by size.
* fract   - fraction of disk that this dir and all of its children consume.  eg if 50%, fract will be 0.5000
* fractcat - fraction category, = fract rounded up to the next highest discrete level is 80, 90, 95, 99, 99.9, 100.  eg if you discard all directories labeled 80, you will clear at least 80% of your space.  You will want to focus on the few labeled 80 or 90 and ignore the many labeled 99.9  or 100.

annot.files.txt files:
* filepath     - full path to the file.  Unruly characters (eg tab, carriage return, newline, quotes, foreign alphabets) are backslash escaped. Space and comma survive.
* size    last_access     last_modified   username        groupname       -- metadata about the file.  File is sorted by size.
* group_readable  all_readable    '1' or '0' to indicate whether others could access the file
* symlink -- always '0'
* nlink       inode   dupe -- fields to help avoid overcounting hard links. When multiple files are hard linked to the same inode (ie nlink>1), then all but one of the dupe fields will be 1 within the same scan.
* filepath_escaped  -- 1 if filename contains backslash escapes, otherwise 0      
* fract   -- fraction of the size of the total disk scan that this file takes up.  Eg a file consuming 1% of the space will have fract=0.0100
* cumfract      --cumulative fraction of disk consumed by this file and all the ones bigger than it. 
* cumfractcat    -- cumulative fraction category, = cumfract rounded up to the next highest discrete level is 80, 90, 95, 99, 99.9, 100.  eg if you discard all files labeled 80, you will clear at least 80% of your space.  You will want to focus on the few labeled 80 or 90 and ignore the many labeled 99.9  or 100.
* ext     ext2    ext3 -- file extensions, double extensions, etc. foo.tar.gz will have: ext=.gz ext2=.gz.tar ext3=NA


To generate these...
catalog_disk_usage.py -o outdir -r root_dir_to_scan [-v]
* intended to be run as root
* generates <scanpath>_<scandate>.files.txt and <scanpath>_<scandate>.dirs.txt
  
 annotate_scan_files.py <scanfile>
 * can run on either .dirs.txt or .files.txt, creates .annot.dirs.txt or .annot.files.txt in the same location
  
 Summarize.py <scanfile>
  * can run on .files.txt (or .annot.files.txt)
  * creates .summ.txt in the same location
  
  
